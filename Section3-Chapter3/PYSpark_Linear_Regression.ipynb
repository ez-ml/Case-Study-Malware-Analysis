{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import socket\n",
    "import pyspark\n",
    "\n",
    "SPARK_NAMESPACE=\"default\"\n",
    "SA=\"spark-driver\"\n",
    "K8S_CACERT=\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n",
    "K8S_TOKEN=\"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n",
    "DOCKER_IMAGE=\"hishailesh77/spark2.4.6-deb:latest\"\n",
    "SPARK_DRIVER_HOST=socket.getfqdn()\n",
    "SPARK_DRIVER_PORT=20020\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter Notebook\") \\\n",
    "    .master(\"k8s://https://kubernetes.default:443\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",SA) \\\n",
    "    .config(\"spark.kubernetes.namespace\",SPARK_NAMESPACE) \\\n",
    "    .config(\"spark.kubernetes.authenticate.subdmission.caCertFile\",K8S_CACERT) \\\n",
    "    .config(\"spark.kubernetes.authenticate.submission.oauthTokenFile\",K8S_TOKEN) \\\n",
    "    .config(\"spark.kubernetes.container.image\", DOCKER_IMAGE) \\\n",
    "    .config(\"spark.kubernetes.container.image.pullPolicy\",\"Always\") \\\n",
    "    .config(\"spark.driver.port\",SPARK_DRIVER_PORT) \\\n",
    "    .config(\"spark.driver.host\",SPARK_DRIVER_HOST) \\\n",
    "    .config(\"spark.executor.instances\", \"12\") \\\n",
    "    .config(\"spark.driver.memory\",\"16g\") \\\n",
    "    .config(\"spark.executor.memory\",\"16g\") \\\n",
    "    .config(\"spark.driver.cores\",\"8\") \\\n",
    "    .config(\"spark.executor.cores\",\"4\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"8g\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAX3D75DYHLQPYD4IT\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"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\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\",\"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\",\"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc= spark.sparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"parquet\").option(\"inferSchema\", \"true\").option(\"header\", \"false\").load(\"s3a://ml-workflow-data/security/Malware_Dataset/Output/malware_tmp_parquet_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+\n",
      "|           file_name|                File|label|              countv|            features|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+\n",
      "|13YpdP5vTLOazSQFRgJn|s3a://ml-workflow...|    3|(20,[0,1,3,4,5,6,...|[1094.0,564.0,0.0...|\n",
      "|12tjh4qCkcHpObVBEeMr|s3a://ml-workflow...|    3|(20,[0,1,2,3,4,5,...|[2843.0,2300.0,86...|\n",
      "|15loeAHtkJa8BuFi6Zry|s3a://ml-workflow...|    1|(20,[0,1,2,3,4,5,...|[126025.0,107847....|\n",
      "|4JGbOVQnEt3ZP5acW7Yz|s3a://ml-workflow...|    6|(20,[0,1,2,3,4,5,...|[146066.0,130765....|\n",
      "|5YMeDkHjclrCPd8OuymR|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[76734.0,68712.0,...|\n",
      "|12Jd4qpOzTtQC3E6PXDb|s3a://ml-workflow...|    3|(20,[0,1,3,4,5,6,...|[1095.0,561.0,0.0...|\n",
      "|7KqfVlEBOmr6tTQewNG8|s3a://ml-workflow...|    7|(20,[0,1,2,3,4,5,...|[2387.0,1167.0,48...|\n",
      "|5PBvwNE8sCzm1bjUlf6A|s3a://ml-workflow...|    7|(20,[0,1,2,3,4,5,...|[2384.0,1169.0,49...|\n",
      "|6cylqphx5sHCm4fYBEdK|s3a://ml-workflow...|    7|(20,[0,1,2,3,4,5,...|[2386.0,1174.0,52...|\n",
      "|7J4un8UNCgcPYmzsyxZI|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[94992.0,81582.0,...|\n",
      "|129LWuNOIS0RjQnq5om6|s3a://ml-workflow...|    3|(20,[0,1,3,4,5,6,...|[1066.0,428.0,0.0...|\n",
      "|7Ebd8M0gDHBRIjFvr6SQ|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[97684.0,84637.0,...|\n",
      "|7KHscjvztoka0QpqYFxb|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[39494.0,34922.0,...|\n",
      "|5JfdLYbqX8oFmD1WTg97|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[76719.0,68146.0,...|\n",
      "|6d0uJ9rYK1FcjRimvVNt|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[104409.0,86829.0...|\n",
      "|5QjzywcSG4XLhNRt8unq|s3a://ml-workflow...|    7|(20,[0,1,2,3,4,5,...|[2370.0,1185.0,51...|\n",
      "|5QOWxyz7oqf9T6KIBDbk|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[87480.0,75488.0,...|\n",
      "|3AJgNPo6iqT8BeODfxHK|s3a://ml-workflow...|    8|(20,[0,1,2,3,4,5,...|[16926.0,4007.0,2...|\n",
      "|5JLHiDhkzyYPdTeuMqXb|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[76241.0,68343.0,...|\n",
      "|6WrBNPTRljxmLahZzo5s|s3a://ml-workflow...|    7|(20,[0,1,2,3,4,5,...|[2383.0,1157.0,50...|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7137254901960784 , Test Error =  0.28627450980392155\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "(trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "#predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "#Fit and Transform the data and calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "error=(1.0 -accuracy)\n",
    "\n",
    "\n",
    "print(f\"Accuracy = {accuracy} , Test Error =  {error}\")\n",
    "\n",
    "#Save the Model\n",
    "\n",
    "model.write().overwrite().save(\"s3a://ml-workflow-data/security/Malware_Dataset/Output/Model_LR_MaxITR_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       7.0|    3|[1094.0,564.0,0.0...|\n",
      "|       3.0|    3|[2843.0,2300.0,86...|\n",
      "|       8.0|    1|[126025.0,107847....|\n",
      "|       6.0|    6|[146066.0,130765....|\n",
      "|       4.0|    4|[76734.0,68712.0,...|\n",
      "|       7.0|    3|[1095.0,561.0,0.0...|\n",
      "|       7.0|    7|[2387.0,1167.0,48...|\n",
      "|       7.0|    7|[2384.0,1169.0,49...|\n",
      "|       7.0|    7|[2386.0,1174.0,52...|\n",
      "|       4.0|    4|[94992.0,81582.0,...|\n",
      "|       7.0|    3|[1066.0,428.0,0.0...|\n",
      "|       4.0|    4|[97684.0,84637.0,...|\n",
      "|       4.0|    4|[39494.0,34922.0,...|\n",
      "|       4.0|    4|[76719.0,68146.0,...|\n",
      "|       4.0|    4|[104409.0,86829.0...|\n",
      "|       7.0|    7|[2370.0,1185.0,51...|\n",
      "|       4.0|    4|[87480.0,75488.0,...|\n",
      "|       8.0|    8|[16926.0,4007.0,2...|\n",
      "|       4.0|    4|[76241.0,68343.0,...|\n",
      "|       7.0|    7|[2383.0,1157.0,50...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the Model from S3\n",
    "\n",
    "savedModel = PipelineModel.load(\"s3a://ml-workflow-data/security/Malware_Dataset/Output/Model_LR_MaxITR_100\")\n",
    "new_df=spark.read.format(\"parquet\").option(\"inferSchema\", \"true\").option(\"header\", \"false\").load(\"s3a://ml-workflow-data/security/Malware_Dataset/Output/malware_tmp_parquet_2\")\n",
    "\n",
    "#Predict the new values\n",
    "predictions = savedModel.transform(new_df)\n",
    "\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
