import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.functions._
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, StopWordsRemover, VectorAssembler}
import org.apache.spark.sql.{DataFrame, SparkSession}

object FeatureEngineering2 {

  def featureEngineeringCountV(dataMatrix: DataFrame): DataFrame ={

    val tokenizer = new RegexTokenizer().setInputCol("Text").setOutputCol("wordsArray")
    val remover = new StopWordsRemover().setInputCol("wordsArray").setOutputCol("filteredWords")
    val cnt = new  CountVectorizer().setInputCol("filteredWords").setOutputCol("countv").setVocabSize(20)
    val assembler = new VectorAssembler().setInputCols(Array("countv")).setOutputCol("features")
    val pipeline = new Pipeline().setStages(Array(tokenizer, remover, cnt, assembler))
    val model= pipeline.fit(dataMatrix)
    model.transform(dataMatrix).drop("Text","wordsArray","filteredWords")


  }

  def main(args: Array[String]): Unit = {
    print("Starting Feature Engineering 1")
    print("####################################################################################################################################")
    val spark = SparkSession
      .builder()
      .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
      .config("spark.hadoop.fs.s3a.multiobjectdelete.enable","false")
      .config("spark.hadoop.fs.s3a.fast.upload","true")
      .getOrCreate()

    import spark.implicits._

    val df_1 = spark.read.format("parquet").option("inferSchema", "true").
      option("header", "false").
      load("s3a://ml-workflow-data/security/Malware_Dataset/Output/malware_tmp_parquet_1")

    val df_2 = df_1.repartition(5000)

    val final_df = featureEngineeringCountV(df_2)

    final_df.coalesce(100).write.mode("overwrite").format("parquet").option("compression", "snappy").mode("overwrite").save("s3a://ml-workflow-data/security/Malware_Dataset/Output/malware_tmp_parquet_12")

    val test_df=spark.read.format("parquet").option("inferSchema", "true").
      option("header", "false").
      load("s3a://ml-workflow-data/security/Malware_Dataset/Output/malware_tmp_parquet_12")

    test_df.groupBy("label").count().show()

    print("####################################################################################################################################")

  }

}