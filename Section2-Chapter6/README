Installation on the host

Docker with CUDA

curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub
sudo add-apt-repository "deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /"

sudo apt update

sudo apt install cuda

nvidia-smi


docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f
sudo apt-get purge -y nvidia-docker
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \
sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo pkill -SIGHUP dockerd

sudo nvidia-container-cli --load-kmods info


docker run --runtime=nvidia -it nvidia/cuda nvidia-smi




RAPIDS NLP Ref

import cudf
import os

def join_df(path):
    data = cudf.DataFrame()
    for file in os.listdir(path):
        print(f"In path : {path}{file}")
        temp = cudf.read_csv(path+file)
        temp = temp[temp.lang=='en']
        data = cudf.concat([data,temp])
    return data

df = join_df('tweets/')

tweets = df['text']
from cuml.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer(stop_words='english')
tfidf_matrix = vec.fit_transform(tweets)



from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
STOPWORDS = nltk.corpus.stopwords.words('english')

filters = [ '!', '"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/',  '\\', ':', ';', '<', '=', '>',
           '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\t','\n',"'",",",'~' , 'â€”']

def preprocess_text(input_strs , filters=None , stopwords=STOPWORDS):
    """
        * filter punctuation
        * to_lower
        * remove stop words (from nltk corpus)
        * remove multiple spaces with one
        * remove leading spaces
    """

    # filter punctuation and case conversion
    translation_table = {ord(char): ord(' ') for char in filters}
    input_strs = input_strs.str.translate(translation_table)
    input_strs = input_strs.str.lower()

    # remove stopwords
    stopwords_gpu = cudf.Series(stopwords)
    input_strs =  input_strs.str.replace_tokens(STOPWORDS, ' ')

    # replace multiple spaces with single one and strip leading/trailing spaces
    input_strs = input_strs.str.normalize_spaces( )
    input_strs = input_strs.str.strip(' ')

    return input_strs

def preprocess_text_df(df, text_cols=['text'], **kwargs):
    for col in text_cols:
        df[col] = preprocess_text(df[col], **kwargs)
    return  df


%time df = preprocess_text_df(df_malware, filters=filters)



df.to_parquet('malware_parquet.snappy')

df_malware=cudf.read_parquet('malware_parquet.snappy')





https://www.opencodez.com/how-to-guide/text-classification-using-keras.htm

https://docs.rapids.ai/api/cudf/stable/10min.html
https://gist.github.com/Garfounkel/e96f2f48d1de35b21506a13cdc37a363
https://medium.com/rapids-ai/natural-language-processing-text-preprocessing-and-vectorizing-at-rocking-speed-with-rapids-cuml-74b8d751812e
https://medium.com/rapids-ai/gpu-text-processing-now-even-simpler-and-faster-bde7e42c8c8a

https://medium.com/rapids-ai/show-me-the-word-count-3146e1173801


