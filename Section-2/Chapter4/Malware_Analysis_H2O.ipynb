{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "SPARK_NAMESPACE=\"default\"\n",
    "SA=\"spark-driver\"\n",
    "K8S_CACERT=\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n",
    "K8S_TOKEN=\"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n",
    "DOCKER_IMAGE=\"hishailesh77/spark-h2o:latest\"\n",
    "SPARK_DRIVER_HOST=socket.getfqdn()\n",
    "SPARK_DRIVER_PORT=20020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://192.168.1.30:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>08 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.30.1.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>21 days, 2 hours and 1 minute </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>sparkling-water-root_spark-application-1600980689473</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>10</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>152.7 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>40</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>40</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://192.168.1.30:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>null</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>XGBoost, Algos, Amazon S3, Sparkling Water REST API Extensions, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.7.3 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------------------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         08 secs\n",
       "H2O_cluster_timezone:       Etc/UTC\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.30.1.2\n",
       "H2O_cluster_version_age:    21 days, 2 hours and 1 minute\n",
       "H2O_cluster_name:           sparkling-water-root_spark-application-1600980689473\n",
       "H2O_cluster_total_nodes:    10\n",
       "H2O_cluster_free_memory:    152.7 Gb\n",
       "H2O_cluster_total_cores:    40\n",
       "H2O_cluster_allowed_cores:  40\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://192.168.1.30:54321\n",
       "H2O_connection_proxy:       null\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         XGBoost, Algos, Amazon S3, Sparkling Water REST API Extensions, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.7.3 final\n",
       "--------------------------  -------------------------------------------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * Sparkling Water Version: 3.30.1.2-1-3.0\n",
      " * H2O name: root\n",
      " * cluster size: 10\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (0,192.168.1.30,54321)\n",
      "  (1,192.168.0.75,54321)\n",
      "  (2,192.168.2.83,54321)\n",
      "  (3,192.168.1.106,54321)\n",
      "  (4,192.168.2.107,54321)\n",
      "  (5,192.168.1.119,54321)\n",
      "  (6,192.168.1.127,54321)\n",
      "  (7,192.168.1.172,54321)\n",
      "  (8,192.168.0.211,54321)\n",
      "  (9,192.168.0.235,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://jupyter-lab-host.jupyter-headless.default.svc.cluster.local:54321 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pysparkling import *\n",
    "from pysparkling.ml import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"Spark-Spylon\") \\\n",
    ".master(\"k8s://https://kubernetes.default:443\") \\\n",
    ".config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",SA) \\\n",
    ".config(\"spark.kubernetes.namespace\",SPARK_NAMESPACE) \\\n",
    ".config(\"spark.kubernetes.authenticate.subdmission.caCertFile\",K8S_CACERT) \\\n",
    ".config(\"spark.kubernetes.authenticate.submission.oauthTokenFile\",K8S_TOKEN) \\\n",
    ".config(\"spark.kubernetes.container.image\", DOCKER_IMAGE) \\\n",
    ".config(\"spark.kubernetes.container.image.pullPolicy\",\"Always\") \\\n",
    ".config(\"spark.driver.port\",SPARK_DRIVER_PORT) \\\n",
    ".config(\"spark.driver.host\",SPARK_DRIVER_HOST) \\\n",
    ".config(\"spark.executor.instances\", \"10\") \\\n",
    ".config(\"spark.driver.memory\",\"16g\") \\\n",
    ".config(\"spark.executor.memory\",\"16g\") \\\n",
    ".config(\"spark.driver.cores\",\"8\") \\\n",
    ".config(\"spark.executor.cores\",\"4\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAX3D75DYHLQPYD4IT\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.secret.key\", \"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\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\",\"false\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.fast.upload\",\"true\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "sc= spark.sparkContext.getOrCreate()\n",
    "\n",
    "hc = H2OContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"parquet\").option(\"inferSchema\", \"true\"). \\\n",
    "    option(\"header\", \"false\"). \\\n",
    "    load(\"s3a://ml-workflow-data/security/Malware_Dataset/Output/malware_tmp_parquet_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoML=H2OAutoML(maxRuntimeSecs=60,splitRatio=0.9,labelCol=\"label\",excludeAlgos = [\"GLM\", \"DeepLearning\", \"DRF\"], maxModels=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[autoML])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2oModel=pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------------+------------------+------------------+------------------+-------------------+\n",
      "|   |            model_id|mean_residual_deviance|              rmse|               mse|               mae|              rmsle|\n",
      "+---+--------------------+----------------------+------------------+------------------+------------------+-------------------+\n",
      "|  0|StackedEnsemble_A...|     1.384992428277505| 1.176857012672952| 1.384992428277505|0.6002382939123094|0.26140424113853006|\n",
      "|  1|StackedEnsemble_B...|    1.4242675043275796|1.1934267905186222|1.4242675043275796|0.6252748222287957|0.26723081278107935|\n",
      "|  2|XGBoost_grid__1_A...|     1.549920152429178| 1.244957891829751| 1.549920152429178| 0.649358882694631| 0.2669420901334468|\n",
      "|  3|GBM_grid__1_AutoM...|    2.0784471979536643|1.4416820724257011|2.0784471979536643|1.0610448205829806|0.32615569797235566|\n",
      "|  4|GBM_4_AutoML_2020...|     4.740270932513955|2.1772163265311866| 4.740270932513955|1.9444324105570916| 0.4589623543193459|\n",
      "|  5|GBM_2_AutoML_2020...|     4.770451591834756|2.1841363491858186| 4.770451591834756| 1.962342249325852|  0.460821906895443|\n",
      "|  6|GBM_3_AutoML_2020...|     5.080469465141892| 2.253989677248299| 5.080469465141892|2.0198880896344202|0.47089685000268033|\n",
      "|  7|GBM_1_AutoML_2020...|     6.241337883036235|2.4982669759327636| 6.241337883036235|2.2493912623861676| 0.5120199676488182|\n",
      "|  8|GBM_5_AutoML_2020...|     6.738537679602771|2.5958693494863665| 6.738537679602771|2.3252489877346703| 0.5262773650106523|\n",
      "|  9|XGBoost_3_AutoML_...|     12.72942346068939|3.5678317590224724| 12.72942346068939|2.9387558201277577| 0.6857415117837223|\n",
      "| 10|XGBoost_1_AutoML_...|    14.893806807420484|3.8592495134961777|14.893806807420484|3.2239141575992107| 0.7644590591654804|\n",
      "| 11|XGBoost_2_AutoML_...|    14.900121690610604|3.8600675759124483|14.900121690610604|3.2326717832805336| 0.7613564517824778|\n",
      "+---+--------------------+----------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoML.getLeaderboard().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = h2oModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+------------------+\n",
      "|           file_name|                File|label|              countv|            features| detailed_prediction|        prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+------------------+\n",
      "|18T9jLsd0DZQc2aRrSpb|s3a://ml-workflow...|    3|(20,[0,1,3,4,5,6,...|[1078.0,448.0,0.0...|[3.5531533980284182]|3.5531533980284182|\n",
      "|18eZt9qWksQhoY3K60aE|s3a://ml-workflow...|    2|(20,[0,1,2,3,4,5,...|[885721.0,529119....|[2.6782276598607986]|2.6782276598607986|\n",
      "|19BtdpKYGOSyX3oZsirc|s3a://ml-workflow...|    3|(20,[0,1,2,3,4,5,...|[1086.0,676.0,123...|[2.8085891659890816]|2.8085891659890816|\n",
      "|19zYbuW3XONcEedv7xUl|s3a://ml-workflow...|    1|(20,[0,1,2,3,4,5,...|[2433.0,3388.0,54...| [1.319887144755942]| 1.319887144755942|\n",
      "|1A7UqoIMrxHgVuW3FS9c|s3a://ml-workflow...|    6|(20,[0,1,2,3,4,5,...|[18150.0,3562.0,6...| [5.977480674397662]| 5.977480674397662|\n",
      "|1OTMQ5zCn6hZGPdBcEje|s3a://ml-workflow...|    2|(20,[0,1,2,3,4,5,...|[80570.0,50741.0,...| [1.732446578040876]| 1.732446578040876|\n",
      "|8KNqArVDfusEobR4Y0lO|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[76170.0,69102.0,...| [4.127705648198009]| 4.127705648198009|\n",
      "|8NOJxRZ65rjy1H0pKWsA|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[85711.0,73970.0,...| [4.026831242659882]| 4.026831242659882|\n",
      "|8dW2hOcgRfSTnapKVHwl|s3a://ml-workflow...|    7|(20,[0,1,2,3,4,5,...|[2385.0,1151.0,50...| [6.961989502067767]| 6.961989502067767|\n",
      "|8hPsWwjKD51Z7BtmLNx4|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[280244.0,242019....|[4.0308474299560695]|4.0308474299560695|\n",
      "|8vJiQURcq15ZtmEdHOIp|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[47474.0,40929.0,...| [4.830821836918386]| 4.830821836918386|\n",
      "|a8hZBDrjGiMdtzSklFNV|s3a://ml-workflow...|    7|(20,[0,1,2,3,4,5,...|[12930.0,12598.0,...|[3.6331931028240634]|3.6331931028240634|\n",
      "|adQni6wWvTPIkf0xsZy7|s3a://ml-workflow...|    7|(20,[0,1,2,3,4,5,...|[2391.0,1172.0,52...| [6.831202775097663]| 6.831202775097663|\n",
      "|bGPHZFpAL3N957064wzj|s3a://ml-workflow...|    5|(20,[0,1,2,3,4,5,...|[8157.0,5811.0,26...| [5.302853181572224]| 5.302853181572224|\n",
      "|01kcPWA9K2BOxQeS5Rju|s3a://ml-workflow...|    1|(20,[0,1,2,3,4,5,...|[141.0,480.0,7.0,...| [2.944950157988142]| 2.944950157988142|\n",
      "|0AnoOZDNbPXIr2MRBSCJ|s3a://ml-workflow...|    1|(20,[0,1,2,3,4,5,...|[3314.0,3023.0,63...|[1.4122477329100915]|1.4122477329100915|\n",
      "|0DqUX5rkg3IbMY6BLGCE|s3a://ml-workflow...|    1|(20,[0,1,2,3,4,5,...|[9126.0,8106.0,52...|[1.3126038652519307]|1.3126038652519307|\n",
      "|0W1RChtwZvj4Qy78GYUJ|s3a://ml-workflow...|    2|(20,[0,1,2,3,4,5,...|[48646.0,40129.0,...|[3.8086275168124084]|3.8086275168124084|\n",
      "|0XSHfwZAgYm7eFVInuDT|s3a://ml-workflow...|    3|(20,[0,1,2,3,4,5,...|[1069.0,681.0,124...|[2.9174868514665153]|2.9174868514665153|\n",
      "|0XVbihU4clgRS9A6tsB2|s3a://ml-workflow...|    2|(20,[0,1,2,3,4,5,...|[25719.0,17262.0,...|[1.9351972021246517]|1.9351972021246517|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as func\n",
    "data_df = predictions.withColumn(\"prediction\",func.round(predictions[\"prediction\"], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|           file_name|                File|label|              countv|            features| detailed_prediction|prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|18T9jLsd0DZQc2aRrSpb|s3a://ml-workflow...|    3|(20,[0,1,3,4,5,6,...|[1078.0,448.0,0.0...|[3.5531533980284182]|       4.0|\n",
      "|18eZt9qWksQhoY3K60aE|s3a://ml-workflow...|    2|(20,[0,1,2,3,4,5,...|[885721.0,529119....|[2.6782276598607986]|       3.0|\n",
      "|19BtdpKYGOSyX3oZsirc|s3a://ml-workflow...|    3|(20,[0,1,2,3,4,5,...|[1086.0,676.0,123...|[2.8085891659890816]|       3.0|\n",
      "|19zYbuW3XONcEedv7xUl|s3a://ml-workflow...|    1|(20,[0,1,2,3,4,5,...|[2433.0,3388.0,54...| [1.319887144755942]|       1.0|\n",
      "|1A7UqoIMrxHgVuW3FS9c|s3a://ml-workflow...|    6|(20,[0,1,2,3,4,5,...|[18150.0,3562.0,6...| [5.977480674397662]|       6.0|\n",
      "|1OTMQ5zCn6hZGPdBcEje|s3a://ml-workflow...|    2|(20,[0,1,2,3,4,5,...|[80570.0,50741.0,...| [1.732446578040876]|       2.0|\n",
      "|8KNqArVDfusEobR4Y0lO|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[76170.0,69102.0,...| [4.127705648198009]|       4.0|\n",
      "|8NOJxRZ65rjy1H0pKWsA|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[85711.0,73970.0,...| [4.026831242659882]|       4.0|\n",
      "|8dW2hOcgRfSTnapKVHwl|s3a://ml-workflow...|    7|(20,[0,1,2,3,4,5,...|[2385.0,1151.0,50...| [6.961989502067767]|       7.0|\n",
      "|8hPsWwjKD51Z7BtmLNx4|s3a://ml-workflow...|    4|(20,[0,1,2,3,4,5,...|[280244.0,242019....|[4.0308474299560695]|       4.0|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7116564417177914"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\" , metricName=\"accuracy\")\n",
    "evaluator.evaluate(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2oModel.write().overwrite().save(\"s3a://ml-workflow-data/security/Malware_Dataset/Output/H2O_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2325.load.\n: java.lang.NullPointerException: null uri host.\n\tat java.util.Objects.requireNonNull(Objects.java:228)\n\tat org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:73)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.setUri(S3AFileSystem.java:470)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:235)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat ai.h2o.sparkling.utils.SparkSessionUtils$.readHDFSFile(SparkSessionUtils.scala:56)\n\tat ai.h2o.sparkling.ml.models.H2OMOJOReader.load(H2OMOJOReader.scala:31)\n\tat ai.h2o.sparkling.ml.models.H2OMOJOReader.load(H2OMOJOReader.scala:26)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:162)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:157)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:162)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:157)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-dd2e7a1f517e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msavedModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://ml-workflow-data/security/Malware_Dataset/Output/H2O_100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"false\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://ml-workflow-data/security/Malware_Dataset/Output/malware_tmp_parquet_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultParamsReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'language'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paramMap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paramMap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaMLReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineSharedReadWrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_from_java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2325.load.\n: java.lang.NullPointerException: null uri host.\n\tat java.util.Objects.requireNonNull(Objects.java:228)\n\tat org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:73)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.setUri(S3AFileSystem.java:470)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:235)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat ai.h2o.sparkling.utils.SparkSessionUtils$.readHDFSFile(SparkSessionUtils.scala:56)\n\tat ai.h2o.sparkling.ml.models.H2OMOJOReader.load(H2OMOJOReader.scala:31)\n\tat ai.h2o.sparkling.ml.models.H2OMOJOReader.load(H2OMOJOReader.scala:26)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:162)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:157)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:162)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:157)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "savedModel = PipelineModel.load(\"s3a://ml-workflow-data/security/Malware_Dataset/Output/H2O_100\")\n",
    "new_df=spark.read.format(\"parquet\").option(\"inferSchema\", \"true\").option(\"header\", \"false\").load(\"s3a://ml-workflow-data/security/Malware_Dataset/Output/malware_tmp_parquet_2\")\n",
    "\n",
    "#Predict the new values\n",
    "predictions = savedModel.transform(new_df)\n",
    "\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "hc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
